{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ne pas oublier d'executer dans le shell avant de lancer python :\n",
    "# source /users/Enseignants/piwowarski/venv/amal/3.7/bin/activate\n",
    "# N.B: commande pour lancer tensorboard: tensorboard --logdir=path/to/log-directory\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Function\n",
    "from torch.autograd import gradcheck\n",
    "from datamaestro import prepare_dataset \n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim import adadelta\n",
    "import ipdb\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TP1 classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Context:\n",
    "    \"\"\"Very simplified context object\"\"\"\n",
    "    def __init__(self):\n",
    "        self._saved_tensors = ()\n",
    "    def save_for_backward(self, *args):\n",
    "        self._saved_tensors = args\n",
    "    @property\n",
    "    def saved_tensors(self):\n",
    "        return self._saved_tensors\n",
    "\n",
    "\n",
    "class linear(Function):\n",
    "##Toute fonction a:\n",
    "## une méthode forward pour calculer l'image de variables et paramètres donnés\n",
    "## une méthode backward pour renvoyer son gradient par rapport à ses variables/paramètres\n",
    "    @staticmethod\n",
    "    #a static method can be called without referring to an object.     \n",
    "    def forward(ctx,x,w,b):\n",
    "        ctx.save_for_backward(x,w,b)\n",
    "        return w*x+b\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx,grad_output):\n",
    "        ## prend comme argument le gradient de l'étage d'au dessus\n",
    "        x,w,b = ctx.saved_tensors\n",
    "        dL_dx=grad_output*w\n",
    "        dL_dw=grad_output*x\n",
    "        dL_db=grad_output\n",
    "        return dL_dx,dL_dw,dL_db\n",
    "    \n",
    "class MSE(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx,y,yhat):\n",
    "        ctx.save_for_backward(y,yhat)\n",
    "        return torch.mean((y-yhat)**2)\n",
    "    @staticmethod\n",
    "    def backward(ctx,grad_output=1):\n",
    "        y,yhat=ctx.saved_tensors\n",
    "        return 2*(y-yhat)*grad_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds=prepare_dataset(\"edu.uci.boston\")\n",
    "fields, data =ds.files.data()\n",
    "n = data.shape[0]\n",
    "\n",
    "def standardize(z):\n",
    "    m=z.mean(dim=0,keepdim=True)\n",
    "    s=z.std(dim=0,keepdim=True)\n",
    "    return (z-m)/s\n",
    "\n",
    "x=standardize(torch.tensor(data[:,:-1],dtype=torch.float32))\n",
    "y=standardize(torch.tensor(data[:,-1],dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Différentiation automatique "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linReg(x,y,learning_rate=0.001,epochs=100):\n",
    "\n",
    "    w=torch.randn(x.shape[1],requires_grad=True,dtype=x.dtype)\n",
    "    b=torch.randn(1,requires_grad=True,dtype=x.dtype)\n",
    "    writer=SummaryWriter()\n",
    "    for i in range(epochs):\n",
    "        for j in range(len(x)):\n",
    "            w.requires_grad=True\n",
    "            b.requires_grad=True\n",
    "            x_j=x[j,:]\n",
    "            y_j=y[j]\n",
    "#             ipdb.set_trace()\n",
    "            y_hat=torch.dot(w,x_j)+b\n",
    "            loss=(y_j-y_hat)**2\n",
    "            loss.backward()\n",
    "            ipdb.set_trace()\n",
    "            w=w-learning_rate*w.grad\n",
    "            #met à jour w,b en fonction d'eux même (impossible?)             \n",
    "            b=b-learning_rate*b.grad\n",
    "            w.grad.data.zero_()\n",
    "            b.grad.data.zero_()\n",
    "        writer.add_scalar('Loss LinReg',loss,i)\n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# w,b=linReg(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimiseur "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Epoch 100\n",
      "Epoch 200\n",
      "Epoch 300\n",
      "Epoch 400\n",
      "Epoch 500\n",
      "Epoch 600\n",
      "Epoch 700\n",
      "Epoch 800\n",
      "Epoch 900\n"
     ]
    }
   ],
   "source": [
    "learning_rate=1e-2\n",
    "nb_epochs=1000\n",
    "\n",
    "def f(x,w,b):\n",
    "    return torch.mv(x,w)+b\n",
    "\n",
    "def MSE(y,y_hat):\n",
    "    return torch.mean(torch.pow((y-y_hat),2))\n",
    "\n",
    "\n",
    "w=torch.nn.Parameter(torch.randn(x.shape[1]))\n",
    "b=torch.nn.Parameter(torch.randn(1))\n",
    "\n",
    "optim=torch.optim.SGD(params=[w,b],lr=learning_rate)\n",
    "# Configuration de l'optimiseur: paramètres et critère d'arrêt\n",
    "optim.zero_grad()\n",
    "# Réinitialisation du gradient\n",
    "\n",
    "writer=SummaryWriter()\n",
    "for i in range(nb_epochs):\n",
    "    index=torch.randint(0,len(x),size=(1,))\n",
    "    #Forward     \n",
    "    loss=MSE(f(x[index,:],w,b),y[index])\n",
    "    writer.add_scalar(\"SGD loss\",loss,i)\n",
    "    #loss_history.append(loss)\n",
    "    #Backward\n",
    "    loss.backward()\n",
    "    #Mise à jour des paramètres à chaque epoch (mod 1)\n",
    "    if i % 1==0:\n",
    "        optim.step() #met à jour les paramètres [w,b]\n",
    "        optim.zero_grad() #réinitialise le gradient\n",
    "    if i%100==0:\n",
    "        print(\"Epoch\",i)\n",
    "        \n",
    "# plt.plot(loss_history)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0\n",
      "Epoch  100\n",
      "Epoch  200\n",
      "Epoch  300\n",
      "Epoch  400\n",
      "Epoch  500\n",
      "Epoch  600\n",
      "Epoch  700\n",
      "Epoch  800\n",
      "Epoch  900\n"
     ]
    }
   ],
   "source": [
    "nb_epochs=1000\n",
    "learning_rate=1e-3\n",
    "\n",
    "def f(x,w,b):\n",
    "#     ipdb.set_trace()\n",
    "    return torch.mv(x,w)+b\n",
    "\n",
    "def MSE(y,y_hat):\n",
    "    return torch.mean(torch.pow((y-y_hat),2))\n",
    "\n",
    "w=torch.nn.Parameter(torch.randn(x.shape[1]))\n",
    "b=torch.nn.Parameter(torch.randn(1))\n",
    "\n",
    "optim=torch.optim.SGD(params=[w,b],lr=learning_rate)\n",
    "# Configuration de l'optimiseur\n",
    "optim.zero_grad()\n",
    "# Réinitialisation du gradient\n",
    "\n",
    "writer=SummaryWriter()\n",
    "for i in range(nb_epochs):\n",
    "#     for j in range(len(x)):\n",
    "    #Forward     \n",
    "    loss=MSE(f(x,w,b),y)\n",
    "    #Backward\n",
    "    loss.backward()\n",
    "    writer.add_scalar(\"Batch loss\",loss,i)\n",
    "    if i % 1==0:\n",
    "        optim.step() #met à jour les paramètres [w,b]\n",
    "        optim.zero_grad() #réinitialise le gradient\n",
    "    if i%100==0:\n",
    "        print(\"Epoch \",i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mini-Batch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0\n",
      "Epoch  100\n",
      "Epoch  200\n",
      "Epoch  300\n",
      "Epoch  400\n",
      "Epoch  500\n",
      "Epoch  600\n",
      "Epoch  700\n",
      "Epoch  800\n",
      "Epoch  900\n"
     ]
    }
   ],
   "source": [
    "nb_epochs=1000\n",
    "batch_size=10\n",
    "\n",
    "def f(x,w,b):\n",
    "#     ipdb.set_trace()\n",
    "    return torch.mv(x,w)+b\n",
    "\n",
    "def MSE(y,y_hat):\n",
    "    return torch.mean(torch.pow((y-y_hat),2))\n",
    "\n",
    "w=torch.nn.Parameter(torch.randn(x.shape[1]))\n",
    "b=torch.nn.Parameter(torch.randn(1))\n",
    "\n",
    "optim=adadelta.Adadelta(params=[w,b])\n",
    "# Configuration de l'optimiseur\n",
    "optim.zero_grad()\n",
    "# Réinitialisation du gradient\n",
    "\n",
    "writer=SummaryWriter()\n",
    "for i in range(nb_epochs):\n",
    "    index=torch.randint(0,len(x),size=(batch_size,))\n",
    "    #Forward     \n",
    "    loss=MSE(f(x[index,:],w,b),y[index])\n",
    "    #Backward\n",
    "    loss.backward()\n",
    "    writer.add_scalar(\"MiniBatch loss\",loss,i)\n",
    "    if i % 1==0:\n",
    "        optim.step() #met à jour les paramètres [w,b]\n",
    "        optim.zero_grad() #réinitialise le gradient\n",
    "    if i%100==0:\n",
    "        print(\"Epoch \",i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-layers NN (without container) (SGD algorithm method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0\n",
      "Epoch  100\n",
      "Epoch  200\n",
      "Epoch  300\n",
      "Epoch  400\n",
      "Epoch  500\n",
      "Epoch  600\n",
      "Epoch  700\n",
      "Epoch  800\n",
      "Epoch  900\n"
     ]
    }
   ],
   "source": [
    "nb_epochs=1000\n",
    "learning_rate=1e-3\n",
    "\n",
    "f1=torch.nn.Linear(x.shape[1],1)\n",
    "tanh=torch.nn.Tanh()\n",
    "f2=torch.nn.Linear(1,1)\n",
    "mse=torch.nn.MSELoss()\n",
    "\n",
    "# Set optimizer\n",
    "optim=torch.optim.SGD(params=[f1.weight,f1.bias,f2.weight,f2.bias],lr=learning_rate)\n",
    "optim.zero_grad()\n",
    "\n",
    "writer=SummaryWriter()\n",
    "for i in range(nb_epochs):\n",
    "    index=torch.randint(0,len(x),size=(1,))\n",
    "    y1=f1(x[index,:])\n",
    "    z=tanh(y1)\n",
    "    y2=f2(z)\n",
    "    loss=mse(y[index],y2)\n",
    "    loss.backward()\n",
    "    writer.add_scalar(\"SGD loss\",loss.item(),i)\n",
    "    optim.step()\n",
    "    optim.zero_grad()\n",
    "    if i %100==0:\n",
    "        print(\"Epoch \",i)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Container (Mini batch used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0\n",
      "Epoch  100\n",
      "Epoch  200\n",
      "Epoch  300\n",
      "Epoch  400\n",
      "Epoch  500\n",
      "Epoch  600\n",
      "Epoch  700\n",
      "Epoch  800\n",
      "Epoch  900\n"
     ]
    }
   ],
   "source": [
    "nb_epochs=1000\n",
    "batch_size=10\n",
    "learning_rate=1e-2\n",
    "\n",
    "f1=torch.nn.Linear(x.shape[1],1)\n",
    "tanh=torch.nn.Tanh()\n",
    "f2=torch.nn.Linear(1,1)\n",
    "mse=torch.nn.MSELoss()\n",
    "\n",
    "neural_network=torch.nn.Sequential(f1,tanh,f2)\n",
    "\n",
    "optim=torch.optim.SGD(params=list(neural_network.parameters()),lr=learning_rate)\n",
    "for i in range(nb_epochs):\n",
    "    index=torch.randint(0,len(x),size=(batch_size,))\n",
    "    loss=mse(neural_network(x[index,:]),y[index].reshape(len(index),1))\n",
    "    loss.backward()\n",
    "    writer.add_scalar(\"MiniBatch loss\",loss.item(),i)\n",
    "    if i%100==0:\n",
    "        print(\"Epoch \",i)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
