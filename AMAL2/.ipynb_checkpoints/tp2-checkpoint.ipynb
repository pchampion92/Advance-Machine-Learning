{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ne pas oublier d'executer dans le shell avant de lancer python :\n",
    "# source /users/Enseignants/piwowarski/venv/amal/3.7/bin/activate\n",
    "# N.B: commande pour lancer tensorboard: tensorboard --logdir=path/to/log-directory\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Function\n",
    "from torch.autograd import gradcheck\n",
    "from datamaestro import prepare_dataset \n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim import adadelta\n",
    "import ipdb\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TP1 classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Context:\n",
    "    \"\"\"Very simplified context object\"\"\"\n",
    "    def __init__(self):\n",
    "        self._saved_tensors = ()\n",
    "    def save_for_backward(self, *args):\n",
    "        self._saved_tensors = args\n",
    "    @property\n",
    "    def saved_tensors(self):\n",
    "        return self._saved_tensors\n",
    "\n",
    "\n",
    "class linear(Function):\n",
    "##Toute fonction a:\n",
    "## une méthode forward pour calculer l'image de variables et paramètres donnés\n",
    "## une méthode backward pour renvoyer son gradient par rapport à ses variables/paramètres\n",
    "    @staticmethod\n",
    "    #a static method can be called without referring to an object.     \n",
    "    def forward(ctx,x,w,b):\n",
    "        ctx.save_for_backward(x,w,b)\n",
    "        return w*x+b\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx,grad_output):\n",
    "        ## prend comme argument le gradient de l'étage d'au dessus\n",
    "        x,w,b = ctx.saved_tensors\n",
    "        dL_dx=grad_output*w\n",
    "        dL_dw=grad_output*x\n",
    "        dL_db=grad_output\n",
    "        return dL_dx,dL_dw,dL_db\n",
    "    \n",
    "class MSE(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx,y,yhat):\n",
    "        ctx.save_for_backward(y,yhat)\n",
    "        return torch.mean((y-yhat)**2)\n",
    "    @staticmethod\n",
    "    def backward(ctx,grad_output=1):\n",
    "        y,yhat=ctx.saved_tensors\n",
    "        return 2*(y-yhat)*grad_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds=prepare_dataset(\"edu.uci.boston\")\n",
    "fields, data =ds.files.data()\n",
    "n = data.shape[0]\n",
    "\n",
    "def standardize(z):\n",
    "    m=z.mean(dim=0,keepdim=True)\n",
    "    s=z.std(dim=0,keepdim=True)\n",
    "    return (z-m)/s\n",
    "\n",
    "x=standardize(torch.tensor(data[:,:-1],dtype=torch.float32))\n",
    "y=standardize(torch.tensor(data[:,-1],dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Différentiation automatique "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linReg(x,y,learning_rate=0.001,epochs=100):    \n",
    "    w=torch.randn(x.shape[1],requires_grad=True,dtype=torch.double)\n",
    "    b=torch.randn(1,requires_grad=True,dtype=torch.double)\n",
    "    writer=SummaryWriter()\n",
    "    for i in range(epochs):\n",
    "        for j in range(len(x)):\n",
    "            x_j=x[j,:]\n",
    "            y_j=y[j,:]\n",
    "            y_hat=torch.dot(w,x_j)+b\n",
    "            loss=(y_j-y_hat)**2\n",
    "            loss.backward()\n",
    "            ipdb.set_trace()\n",
    "            w=w-learning_rate*w.grad\n",
    "            #met à jour w,b en fonction d'eux même (impossible?)             \n",
    "            b=b-learning_rate*b.grad\n",
    "            w.grad.data.zero_()\n",
    "            b.grad.data.zero_()\n",
    "        writer.add_scalar('Loss LinReg',loss,i)\n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# w,b=linReg(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimiseur "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Epoch 100\n",
      "Epoch 200\n",
      "Epoch 300\n",
      "Epoch 400\n",
      "Epoch 500\n",
      "Epoch 600\n",
      "Epoch 700\n",
      "Epoch 800\n",
      "Epoch 900\n"
     ]
    }
   ],
   "source": [
    "learning_rate=1e-2\n",
    "nb_epochs=1000\n",
    "\n",
    "def f(x,w,b):\n",
    "    return torch.mv(x,w)+b\n",
    "\n",
    "def MSE(y,y_hat):\n",
    "    return torch.mean(torch.pow((y-y_hat),2))\n",
    "\n",
    "\n",
    "w=torch.nn.Parameter(torch.randn(x.shape[1]))\n",
    "b=torch.nn.Parameter(torch.randn(1))\n",
    "\n",
    "optim=torch.optim.SGD(params=[w,b],lr=learning_rate)\n",
    "# Configuration de l'optimiseur: paramètres et critère d'arrêt\n",
    "optim.zero_grad()\n",
    "# Réinitialisation du gradient\n",
    "\n",
    "writer=SummaryWriter()\n",
    "for i in range(nb_epochs):\n",
    "    index=torch.randint(0,len(x),size=(1,))\n",
    "    #Forward     \n",
    "    loss=MSE(f(x[index,:],w,b),y[index])\n",
    "    writer.add_scalar(\"SGD loss\",loss,i)\n",
    "    #loss_history.append(loss)\n",
    "    #Backward\n",
    "    loss.backward()\n",
    "    #Mise à jour des paramètres à chaque epoch (mod 1)\n",
    "    if i % 1==0:\n",
    "        optim.step() #met à jour les paramètres [w,b]\n",
    "        optim.zero_grad() #réinitialise le gradient\n",
    "    if i%100==0:\n",
    "        print(\"Epoch\",i)\n",
    "        \n",
    "# plt.plot(loss_history)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0\n",
      "Epoch  10\n",
      "Epoch  20\n",
      "Epoch  30\n",
      "Epoch  40\n",
      "Epoch  50\n",
      "Epoch  60\n",
      "Epoch  70\n",
      "Epoch  80\n",
      "Epoch  90\n"
     ]
    }
   ],
   "source": [
    "nb_epochs=100\n",
    "\n",
    "def f(x,w,b):\n",
    "#     ipdb.set_trace()\n",
    "    return torch.dot(x,w)+b\n",
    "\n",
    "def MSE(y,y_hat):\n",
    "    return torch.mean(torch.pow((y-y_hat),2))\n",
    "\n",
    "w=torch.nn.Parameter(torch.randn(x.shape[1]))\n",
    "b=torch.nn.Parameter(torch.randn(1))\n",
    "\n",
    "optim=adadelta.Adadelta(params=[w,b])\n",
    "# Configuration de l'optimiseur\n",
    "optim.zero_grad()\n",
    "# Réinitialisation du gradient\n",
    "\n",
    "writer=SummaryWriter()\n",
    "for i in range(nb_epochs):\n",
    "    for j in range(len(x)):\n",
    "        #Forward     \n",
    "        loss=MSE(f(x[j,:],w,b),y[j])\n",
    "        #Backward\n",
    "        loss.backward()\n",
    "    writer.add_scalar(\"Batch loss\",loss,i)\n",
    "    if i % 1==0:\n",
    "        optim.step() #met à jour les paramètres [w,b]\n",
    "        optim.zero_grad() #réinitialise le gradient\n",
    "    if i%10==0:\n",
    "        print(\"Epoch \",i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mini-Batch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0\n",
      "Epoch  10\n",
      "Epoch  20\n",
      "Epoch  30\n",
      "Epoch  40\n",
      "Epoch  50\n",
      "Epoch  60\n",
      "Epoch  70\n",
      "Epoch  80\n",
      "Epoch  90\n"
     ]
    }
   ],
   "source": [
    "nb_epochs=100\n",
    "batch_size=10\n",
    "\n",
    "def f(x,w,b):\n",
    "#     ipdb.set_trace()\n",
    "    return torch.dot(x,w)+b\n",
    "\n",
    "def MSE(y,y_hat):\n",
    "    return torch.mean(torch.pow((y-y_hat),2))\n",
    "\n",
    "w=torch.nn.Parameter(torch.randn(x.shape[1]))\n",
    "b=torch.nn.Parameter(torch.randn(1))\n",
    "\n",
    "optim=adadelta.Adadelta(params=[w,b])\n",
    "# Configuration de l'optimiseur\n",
    "optim.zero_grad()\n",
    "# Réinitialisation du gradient\n",
    "\n",
    "writer=SummaryWriter()\n",
    "for i in range(nb_epochs):\n",
    "    index=torch.randint(0,len(x),size=(batch_size,))\n",
    "    #Forward     \n",
    "    loss=MSE(f(x[j,:],w,b),y[j])\n",
    "    #Backward\n",
    "    loss.backward()\n",
    "    writer.add_scalar(\"MiniBatch loss\",loss,i)\n",
    "    if i % 1==0:\n",
    "        optim.step() #met à jour les paramètres [w,b]\n",
    "        optim.zero_grad() #réinitialise le gradient\n",
    "    if i%10==0:\n",
    "        print(\"Epoch \",i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-layers NN (without container) (SGD algorithm method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  100\n",
      "Epoch  200\n",
      "Epoch  300\n",
      "Epoch  400\n",
      "Epoch  500\n",
      "Epoch  600\n",
      "Epoch  700\n",
      "Epoch  800\n",
      "Epoch  900\n"
     ]
    }
   ],
   "source": [
    "nb_epochs=1000\n",
    "learning_rate=1e-3\n",
    "\n",
    "f1=torch.nn.Linear(x.shape[1],1)\n",
    "tanh=torch.nn.Tanh()\n",
    "f2=torch.nn.Linear(1,1)\n",
    "mse=torch.nn.MSELoss()\n",
    "\n",
    "# Define parameters to optimize\n",
    "w1=f1.weight\n",
    "b1=f1.bias\n",
    "w2=f2.weight\n",
    "b2=f2.bias\n",
    "\n",
    "# Set optimizer\n",
    "optim=torch.optim.SGD(params=[w1,b1,w2,b2],lr=learning_rate)\n",
    "optim.zero_grad()\n",
    "\n",
    "writer=SummaryWriter()\n",
    "for i in range(nb_epochs):\n",
    "    index=torch.randint(0,len(x),size=(1,))\n",
    "    y1=f1(x[index,:])\n",
    "    z=tanh(y1)\n",
    "    y2=f2(z)\n",
    "    loss=mse(y[index],y2)\n",
    "    loss.backward()\n",
    "    writer.add_scalar(\"SGD loss\",loss.item(),i)\n",
    "    optim.step()\n",
    "    f1.weight=w1\n",
    "    f1.bias=b1\n",
    "    f2.weight=w2\n",
    "    f2.bias=b2\n",
    "    optim.zero_grad()\n",
    "    if i %100==0:\n",
    "        print(\"Epoch \",i)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Container (Mini batch used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0\n",
      "Epoch  10\n",
      "Epoch  20\n",
      "Epoch  30\n",
      "Epoch  40\n",
      "Epoch  50\n",
      "Epoch  60\n",
      "Epoch  70\n",
      "Epoch  80\n",
      "Epoch  90\n",
      "Epoch  100\n",
      "Epoch  110\n",
      "Epoch  120\n",
      "Epoch  130\n",
      "Epoch  140\n",
      "Epoch  150\n",
      "Epoch  160\n",
      "Epoch  170\n",
      "Epoch  180\n",
      "Epoch  190\n",
      "Epoch  200\n",
      "Epoch  210\n",
      "Epoch  220\n",
      "Epoch  230\n",
      "Epoch  240\n",
      "Epoch  250\n",
      "Epoch  260\n",
      "Epoch  270\n",
      "Epoch  280\n",
      "Epoch  290\n",
      "Epoch  300\n",
      "Epoch  310\n",
      "Epoch  320\n",
      "Epoch  330\n",
      "Epoch  340\n",
      "Epoch  350\n",
      "Epoch  360\n",
      "Epoch  370\n",
      "Epoch  380\n",
      "Epoch  390\n",
      "Epoch  400\n",
      "Epoch  410\n",
      "Epoch  420\n",
      "Epoch  430\n",
      "Epoch  440\n",
      "Epoch  450\n",
      "Epoch  460\n",
      "Epoch  470\n",
      "Epoch  480\n",
      "Epoch  490\n",
      "Epoch  500\n",
      "Epoch  510\n",
      "Epoch  520\n",
      "Epoch  530\n",
      "Epoch  540\n",
      "Epoch  550\n",
      "Epoch  560\n",
      "Epoch  570\n",
      "Epoch  580\n",
      "Epoch  590\n",
      "Epoch  600\n",
      "Epoch  610\n",
      "Epoch  620\n",
      "Epoch  630\n",
      "Epoch  640\n",
      "Epoch  650\n",
      "Epoch  660\n",
      "Epoch  670\n",
      "Epoch  680\n",
      "Epoch  690\n",
      "Epoch  700\n",
      "Epoch  710\n",
      "Epoch  720\n",
      "Epoch  730\n",
      "Epoch  740\n",
      "Epoch  750\n",
      "Epoch  760\n",
      "Epoch  770\n",
      "Epoch  780\n",
      "Epoch  790\n",
      "Epoch  800\n",
      "Epoch  810\n",
      "Epoch  820\n",
      "Epoch  830\n",
      "Epoch  840\n",
      "Epoch  850\n",
      "Epoch  860\n",
      "Epoch  870\n",
      "Epoch  880\n",
      "Epoch  890\n",
      "Epoch  900\n",
      "Epoch  910\n",
      "Epoch  920\n",
      "Epoch  930\n",
      "Epoch  940\n",
      "Epoch  950\n",
      "Epoch  960\n",
      "Epoch  970\n",
      "Epoch  980\n",
      "Epoch  990\n"
     ]
    }
   ],
   "source": [
    "nb_epochs=1000\n",
    "batch_size=10\n",
    "learning_rate=1e-2\n",
    "\n",
    "f1=torch.nn.Linear(x.shape[1],1)\n",
    "tanh=torch.nn.Tanh()\n",
    "f2=torch.nn.Linear(1,1)\n",
    "mse=torch.nn.MSELoss()\n",
    "\n",
    "neural_network=torch.nn.Sequential(f1,tanh,f2)\n",
    "\n",
    "optim=torch.optim.SGD(params=list(neural_network.parameters()),lr=learning_rate)\n",
    "for i in range(nb_epochs):\n",
    "    index=torch.randint(0,len(x),size=(batch_size,))\n",
    "    loss=mse(neural_network(x[index,:]),y[index].reshape(len(index),1))\n",
    "    loss.backward()\n",
    "    writer.add_scalar(\"MiniBatch loss\",loss.item(),i)\n",
    "    if i%10==0:\n",
    "        print(\"Epoch \",i)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
